
In a CNN model, we start with labeled data (e.g., images or side-channel traces represented as 2D/3D arrays) and split it into training and validation sets along with their labels. The training set is used for learning the parameters of convolutional filters, while the validation set checks the model’s ability to generalize. Training proceeds in batches. For each batch, the data is fed into the network’s first convolutional layer. Here, multiple learnable filters slide (convolve) across the input, computing dot products between the filter weights and small local regions of the input, producing feature maps that highlight different patterns such as edges, textures, or timing patterns. These feature maps are then passed through an activation function (commonly ReLU) to introduce non-linearity, followed by pooling layers (e.g., max pooling) that reduce spatial dimensions while retaining important features. This convolution–activation–pooling sequence may repeat across multiple layers, enabling the network to learn increasingly abstract and complex features.

After the convolutional and pooling layers have extracted and compressed the relevant features, the resulting data is flattened into a 1-dimensional vector and passed through one or more fully connected layers, similar to an MLP. These layers perform a weighted combination of all features and apply activation functions, culminating in an output layer that produces the final prediction (e.g., a probability distribution over classes via softmax). The predicted outputs are then compared with the ground truth labels using a loss function (such as cross-entropy for classification). The loss provides a quantitative measure of how far the predictions are from the actual targets.

Using this loss value, backpropagation is performed throughout the network. In CNNs, this includes computing gradients not only for the weights and biases of the fully connected layers but also for the convolutional filters in earlier layers. The gradients are calculated using the chain rule and propagated backward through the network, layer by layer, including the un-pooling and convolution steps in reverse. An optimizer updates all parameters (filters and fully connected weights) according to these gradients and the chosen learning rate. This forward–loss–backprop–update process repeats for every batch until the entire training set is processed, marking the end of one epoch. After each epoch, the model is evaluated on the validation data using only forward propagation to measure validation loss and accuracy, ensuring that the learned filters and connections generalize to unseen inputs.




In an MLP model, we begin with profiling data and their corresponding labels, which are split into training and validation sets along with their labels. The training set is used to adjust the model’s parameters, while the validation set checks generalization performance. During training, the model processes the data in batches. For each batch, the input features are fed into the input layer, and the data flows through the network layer by layer via forward propagation. At each layer, the neurons perform a weighted sum of inputs, add a bias, and apply an activation function to introduce non-linearity. The final layer produces predictions, which are then compared to the actual labels using a loss function that measures the prediction error.

The computed loss is then used to perform backpropagation, where gradients of the loss with respect to each weight and bias are calculated using the chain rule. These gradients indicate the direction and magnitude of change needed to minimize the error. An optimizer, such as SGD or Adam, updates the parameters based on these gradients and the learning rate. This forward–loss–backprop–update cycle repeats for all batches in the dataset, completing one epoch when the entire training set has been processed. Once an epoch finishes, the model is evaluated on the validation data using only forward propagation to compute validation loss and accuracy, without updating weights, providing insight into how well the model generalizes to unseen data.
