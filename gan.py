# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19nLcU8r3O0xyQsXxBkxFFojrFinkSSpX
"""

''' **15-example mastery journey** for **GAN & CGAN using MLP in TensorFlow**, moving from absolute foundations to advanced applications.
We‚Äôll build you up from ‚Äúwhat the heck is a GAN‚Äù to ‚ÄúI can tune CGANs like a pro,‚Äù with **daily examples**, **real-world parallels**,
**pro tips/hacks**, and **assignments**.
'''

## **Example 1 ‚Äî The Foundation: First GAN with MLP (Random Data)**
### **Goal** : Build the smallest possible GAN using MLPs in TensorFlow to generate 1D points that resemble a target distribution.
## **1. Concept Breakdown**

'''
**GAN Structure:**
1. **Generator (G)** ‚Üí Creates fake data from random noise (`z`).
2. **Discriminator (D)** ‚Üí Distinguishes between real data and fake data.

**Training Process (Adversarial Game):**
* G tries to fool D.
* D tries not to be fooled.
* They train alternately.

**Today‚Äôs Example:**
* Real data: Numbers from a **simple Gaussian distribution**.
* Fake data: Generator‚Äôs output.
* Both G and D will be MLPs with just 1 hidden layer.
'''

## **2. Code**

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# step 1: real data distribution(mean =4, std = 1.25)
def real_data(batch_size):
  return np.random.normal(loc = 4, scale=1.25, size=(batch_size , 1)).astype(np.float32)

real_data(2)

# step2: build the generator
def build_generator():
  model = tf.keras.Sequential([
                              tf.keras.layers.Dense(8, activation='relu', input_shape=(1,)),
                              tf.keras.layers.Dense(1)
                            ])
  return model

# step3: build the discriminator
def build_discriminator():
  model = tf.keras.Sequential([
                              tf.keras.layers.Dense(8, activation='relu', input_shape=(1,)),
                              tf.keras.layers.Dense(1, activation = 'sigmoid')
                            ])
  return model

# step4: create models
generator = build_generator()
discriminator = build_discriminator()

'''
Without calling these functions, we only have the ‚Äúinstructions‚Äù but no actual model in memory.
After calling them, generator and discriminator are ready-to-train TensorFlow models.
'''

#optimizers
g_optimizers = tf.keras.optimizers.Adam(learning_rate = 0.01)
d_optimizers = tf.keras.optimizers.Adam(learning_rate = 0.01)

#Loss
bce = tf.keras.losses.BinaryCrossentropy()

# step 5 : training step
'''
training = false: "Run the model in inference mode(no backprop., no dropping neurons), not training mode."
Keras model automatically understands the training argument ‚Äî you don‚Äôt have to define it in build_generator() yourself.
If you pass training=True, Keras tells all layers inside the model to behave as if training (Dropout, BatchNorm, etc.).
If you pass training=False, it runs in inference mode.
If you don‚Äôt pass anything, Keras will guess based on context (inside a training step vs outside).
'''

def train_step(batch_size):
  real  = real_data(batch_size)
  noise = np.random.uniform(-1,1, size = (batch_size, 1)).astype(np.float32)
  fake  = generator(noise, training = False)

```

---

## **3. Real-World Analogy**

Think of GAN training like a **counterfeit money game**:

* The **Generator** is the counterfeiter.
* The **Discriminator** is the police inspector.
* The counterfeiter keeps improving fake notes to fool the inspector.
* The inspector keeps getting better at spotting fakes.

---

## **4. Pro Hacks for Today**

1. **Small Steps First** ‚Äî Always start with a small, controllable dataset before real images.
2. **Learning Rates** ‚Äî G and D might need different learning rates to stay balanced.
3. **Activation Choice** ‚Äî Avoid `tanh` in D‚Äôs last layer when using BCE loss unless you normalize data.
4. **Noise Input** ‚Äî Uniform noise is fine for simple tasks; Gaussian noise sometimes improves diversity.

---

## **5. Daily Assignment**

* **Modify**: Change the real data distribution to a uniform range \[2, 6] and see if the GAN still learns.
* **Experiment**: Increase hidden layer neurons from 8 to 32 ‚Äî does training stabilize or collapse?
* **Visualize**: Plot G‚Äôs output distribution every 1000 epochs to watch learning in progress.

---

üí° **Tomorrow (Example 2)** ‚Üí We‚Äôll upgrade this GAN to generate **2D points** that follow a circular distribution, introduce **mode collapse detection**, and tweak **loss functions**.

Do you want me to make each example in this series **progressively use more real-world data** so by Example 5 we‚Äôre already working with real small datasets like MNIST? That will make the jump to CGAN smoother.

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Real data distribution (mean=4, std=1.25)
def real_data(batch_size):
    return np.random.normal(loc=4.0, scale=1.25, size=(batch_size, 1)).astype(np.float32)

# --- Step 2: Build the Generator (MLP)
def build_generator():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(8, activation='relu', input_shape=(1,)),
        tf.keras.layers.Dense(1)
    ])
    return model

# --- Step 3: Build the Discriminator (MLP)
def build_discriminator():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(8, activation='relu', input_shape=(1,)),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

# --- Step 4: Create models
generator = build_generator()
discriminator = build_discriminator()

# Optimizers
g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

# Loss
bce = tf.keras.losses.BinaryCrossentropy()

# --- Step 5: Training step
@tf.function
def train_step(batch_size):
    # 1) Train Discriminator
    real = real_data(batch_size)
    noise = np.random.uniform(-1, 1, size=(batch_size, 1)).astype(np.float32)
    fake = generator(noise, training=False)

    with tf.GradientTape() as tape:
        real_loss = bce(tf.ones((batch_size, 1)), discriminator(real, training=True))
        fake_loss = bce(tf.zeros((batch_size, 1)), discriminator(fake, training=True))
        d_loss = real_loss + fake_loss
    grads = tape.gradient(d_loss, discriminator.trainable_variables)
    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))

    # 2) Train Generator
    noise = np.random.uniform(-1, 1, size=(batch_size, 1)).astype(np.float32)
    with tf.GradientTape() as tape:
        fake = generator(noise, training=True)
        g_loss = bce(tf.ones((batch_size, 1)), discriminator(fake, training=True))
    grads = tape.gradient(g_loss, generator.trainable_variables)
    g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))

    return d_loss, g_loss

# --- Step 6: Training Loop
EPOCHS = 5000
for epoch in range(EPOCHS):
    d_loss, g_loss = train_step(batch_size=16)
    if epoch % 1000 == 0:
        print(f"Epoch {epoch} | D Loss: {d_loss.numpy():.4f} | G Loss: {g_loss.numpy():.4f}")

# --- Step 7: Visualize results
noise = np.random.uniform(-1, 1, size=(1000, 1)).astype(np.float32)
generated_samples = generator(noise, training=False)
plt.hist(real_data(1000), bins=20, alpha=0.5, label="Real Data")
plt.hist(generated_samples.numpy(), bins=20, alpha=0.5, label="Generated Data")
plt.legend()
plt.show()