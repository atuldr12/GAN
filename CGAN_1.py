# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18f624dDfhWz9zoXjjhj_8rxvlgGdI4xv
"""

Got it — instead of passing the condition as a **one-hot vector**, we’ll make the generator and discriminator learn an **embedding vector** for each label.
This way, the network learns a dense representation of the condition, which often helps convergence and quality.

I’ll give you the **Session 1 CGAN MLP code with embedding**.

---

## Key Changes from One-Hot to Embedding

1. **Embedding layers** replace one-hot encoding.

   * `Embedding(num_classes, embed_dim)` maps integer labels to a learned dense vector.
2. Embedding outputs are **flattened** (or reshaped) before concatenating with noise or sample features.
3. Labels remain **integer tensors** (shape `(batch, 1)`), not one-hot arrays.

---

```python
# cgan_mlp_session1_embedding.py
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
import matplotlib.pyplot as plt

# --- Data ---
def make_conditional_2d_data(n_per_class=1000, seed=0):
    np.random.seed(seed)
    # class 0
    x0 = np.random.randn(n_per_class, 2) * 0.4 + np.array([-2.0, 0.0])
    y0 = np.zeros((n_per_class, 1), dtype=np.int32)
    # class 1
    x1 = np.random.randn(n_per_class, 2) * 0.4 + np.array([2.0, 0.0])
    y1 = np.ones((n_per_class, 1), dtype=np.int32)
    X = np.vstack([x0, x1]).astype(np.float32)
    Y = np.vstack([y0, y1]).astype(np.int32)
    perm = np.random.permutation(len(X))
    return X[perm], Y[perm]

# --- Models with embeddings ---
def build_generator(latent_dim=8, embed_dim=4, num_classes=2, hidden=64):
    noise_input = layers.Input(shape=(latent_dim,), name='noise')
    label_input = layers.Input(shape=(1,), dtype='int32', name='label')
    label_emb = layers.Embedding(num_classes, embed_dim)(label_input)
    label_emb = layers.Flatten()(label_emb)
    x = layers.Concatenate()([noise_input, label_emb])
    x = layers.Dense(hidden, activation='relu')(x)
    x = layers.Dense(hidden, activation='relu')(x)
    out = layers.Dense(2, activation=None)(x)
    return Model([noise_input, label_input], out, name='generator')

def build_discriminator(embed_dim=4, num_classes=2, hidden=64):
    sample_input = layers.Input(shape=(2,), name='sample')
    label_input = layers.Input(shape=(1,), dtype='int32', name='label')
    label_emb = layers.Embedding(num_classes, embed_dim)(label_input)
    label_emb = layers.Flatten()(label_emb)
    x = layers.Concatenate()([sample_input, label_emb])
    x = layers.Dense(hidden, activation='relu')(x)
    x = layers.Dense(hidden, activation='relu')(x)
    out = layers.Dense(1, activation='sigmoid')(x)
    return Model([sample_input, label_input], out, name='discriminator')

# --- Training ---
def train_cgan(epochs=1000, batch_size=128, latent_dim=8, embed_dim=4, print_every=200):
    X, Y = make_conditional_2d_data()
    dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(2000).batch(batch_size)

    gen = build_generator(latent_dim=latent_dim, embed_dim=embed_dim)
    disc = build_discriminator(embed_dim=embed_dim)

    g_opt = tf.keras.optimizers.Adam(1e-4)
    d_opt = tf.keras.optimizers.Adam(1e-4)
    bce = tf.keras.losses.BinaryCrossentropy()

    @tf.function
    def d_step(real_samples, real_labels):
        batch = tf.shape(real_samples)[0]
        noise = tf.random.normal((batch, latent_dim))
        fake_samples = gen([noise, real_labels], training=True)

        with tf.GradientTape() as tape:
            real_out = disc([real_samples, real_labels], training=True)
            fake_out = disc([fake_samples, real_labels], training=True)
            d_loss_real = bce(tf.ones_like(real_out), real_out)
            d_loss_fake = bce(tf.zeros_like(fake_out), fake_out)
            d_loss = d_loss_real + d_loss_fake
        grads = tape.gradient(d_loss, disc.trainable_variables)
        d_opt.apply_gradients(zip(grads, disc.trainable_variables))
        return d_loss

    @tf.function
    def g_step(batch_labels):
        batch = tf.shape(batch_labels)[0]
        noise = tf.random.normal((batch, latent_dim))
        with tf.GradientTape() as tape:
            fake_samples = gen([noise, batch_labels], training=True)
            fake_out = disc([fake_samples, batch_labels], training=True)
            g_loss = bce(tf.ones_like(fake_out), fake_out)
        grads = tape.gradient(g_loss, gen.trainable_variables)
        g_opt.apply_gradients(zip(grads, gen.trainable_variables))
        return g_loss

    for epoch in range(1, epochs+1):
        d_losses, g_losses = [], []
        for real_samples, real_labels in dataset:
            d_loss = d_step(real_samples, real_labels)
            g_loss = g_step(real_labels)
            d_losses.append(d_loss.numpy())
            g_losses.append(g_loss.numpy())

        if epoch % print_every == 0 or epoch == 1:
            print(f"Epoch {epoch:04d} | D_loss={np.mean(d_losses):.4f} | G_loss={np.mean(g_losses):.4f}")
            visualize_generation(gen, latent_dim, embed_dim)

    return gen, disc

# --- Visualization ---
def visualize_generation(gen, latent_dim, embed_dim, n_samples=200):
    plt.figure(figsize=(6,3))
    for label_val in [0, 1]:
        z = tf.random.normal((n_samples, latent_dim))
        labels = tf.constant([[label_val]] * n_samples, dtype=tf.int32)
        samples = gen([z, labels], training=False).numpy()
        plt.subplot(1,2,label_val+1)
        plt.scatter(samples[:,0], samples[:,1], alpha=0.6, s=5)
        plt.title(f"Generated label={label_val}")
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    gen, disc = train_cgan(epochs=800, batch_size=128, latent_dim=8, embed_dim=4, print_every=200)
    visualize_generation(gen, latent_dim=8, embed_dim=4)
```

---

## Why embedding helps here

* The **embedding layer** allows the model to learn an optimal numeric representation for each label instead of forcing a fixed one-hot mapping.
* For small datasets, this can speed up convergence and produce **tighter, more separated clusters** for each condition.
* Embedding dimensions (e.g., `embed_dim=4`) are tunable — too small loses capacity, too large wastes parameters.

---

If you want, I can **adapt this embedding version for MNIST in Session 2**, so you’ll see how conditional image generation works with MLPs before moving to CNN-based CGANs. That’s where the real fun begins visually.

Do you want me to move straight to **MNIST CGAN with embedding** for Session 2 next?